---
title: "p8105_hw2_fw2394"
author: "Fang Wang"
date: "2024-09-29"
output: github_document
editor_options: 
  chunk_output_type: inline
---
# Problem 1

## Set up the working_directory: 
```{r working_directory}
setwd("/Users/fangwang/Downloads/P8105 Data Science I/p8105_hw2_fw2394")
```

## Load necessary libraries:
```{r library}
library (tidyverse)
library (dplyr)
library (readxl)
```

## Load the dataset:
```{r data_clean}
nyc_transit =
  read_csv ("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") |> janitor::clean_names() |> 
  select (line, station_name, station_latitude, station_longitude, route1:route11, entry, vending, entrance_type, ada) |> 
mutate(
    entry = case_match(
      entry,
      "YES" ~ TRUE,
      "NO" ~ FALSE
    )
  )
str(nyc_transit)
```
**The dataset contains 19 variables, including line, station_name, station_latitude, station_longitude, route1 to route11, entry, vending, entrance_type, and ada. It has a total of 1,868 observations, meaning there are 1,868 rows and 19 columns.So far, I have performed the following steps to clean and organize the data. First, I converted all variable names to lowercase using janitor::clean_names(). Then, I selected the 19 relevant columns using the select() function. Finally, I transformed the entry variable from a character type ("YES" or "NO") to a logical type (TRUE or FALSE) using case_match(). As a result, the dataset is now tidier and more structured compared to the original version. **

## Find how many distinct stations:
```{r distinct_station}
combined_station_line = nyc_transit |> 
  mutate (station_name_line = paste (station_name, line, sep = " ")) 

unique_stations = 
  combined_station_line |> 
  select (station_name_line) |>
  distinct()
nrow(unique_stations)
```

## Find how many stations that are ADA compliant:
```{r ada}
ada_compliant =
  nyc_transit |> 
  filter (ada == "TRUE")
nrow(ada_compliant)
```

## Calcuate the proportion of stations without vending allow entrance:
```{r prop}
proportion_allow_entrance = nyc_transit |> 
  filter(vending == "NO") |>
  summarize(prop = mean(entry == "TRUE"))
print(proportion_allow_entrance)
```

## Reformat data so that route number and route name are distinctive varaibles:
```{r route_number_name}
nyc_transit_long = combined_station_line |> 
  mutate(across(route1:route11, as.character)) |> 
  pivot_longer(
    cols = route1:route11,
    names_to = "route_number",
    values_to = "route_name"
  ) |> 
  select (route_number, route_name, station_name_line, everything())
head(nyc_transit_long)
```

## To find how many distinct stations serve the A train:
```{r A_train}
nyc_transit_long |> 
  filter (route_name == "A") |> 
  distinct(station_name_line) |> 
  count() 
```

## To find how many stations that serve the A train are ADA compliant:
```{r A_train_ADA}
nyc_transit_long |> 
  filter (route_name == "A", ada == "TRUE") |> 
  count() 
```

# Problem 2

## Load dataset "Mr. Trash Wheel" and clean_up:
```{r mr}
mr_trash_wheel = read_excel ("./data/202409 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", skip = 1) |> 
  janitor::clean_names() |> 
  filter (!is.na(dumpster)) |> 
  select (-x15, -x16) |> 
  mutate(sports_balls = as.integer(round(sports_balls)))|> 
  mutate (
    source = "mr_trash_wheel"
  )
```

## Load dataset "Professor Trash Wheel" and clean_up:
```{r professional}
professional_trash_wheel = read_excel ("./data/202409 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel", skip = 1) |> 
  janitor::clean_names() |> 
  filter (!is.na(dumpster)) |> 
  mutate(year = as.character(year)) |> 
   mutate (
    source = "professional_trash_wheel",
    sports_balls = NA
  ) |> 
  select(dumpster, month, year, date, weight_tons, volume_cubic_yards, plastic_bottles,polystyrene, cigarette_butts, glass_bottles, plastic_bags, wrappers, sports_balls, homes_powered, source)
```

## Load dataset "Gwynnda Trash Wheel" and clean_up:
```{r Gwynnda}
gwynnda_trash_wheel <- read_excel("./data/202409 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel", skip = 1)|> 
  janitor::clean_names() |> 
  filter (!is.na(dumpster)) |> 
  mutate(year = as.character(year)) |> 
   mutate (
    source = "gwynnda_trash_wheel",
    glass_bottles = NA,
    sports_balls = NA
  ) |> 
  select(dumpster, month, year, date, weight_tons, volume_cubic_yards, plastic_bottles,polystyrene, cigarette_butts, glass_bottles, plastic_bags, wrappers, sports_balls, homes_powered, source)
```

## Combine Mr. Trash Wheel and Professional Trash Wheel:
```{r combined_mr_prof}
mr_prof_gwy = rbind (mr_trash_wheel, 
                     professional_trash_wheel, 
                     gwynnda_trash_wheel)
```
**The final dataset "mr_prof_gwy" contains 1033 observations and 15 variables including additional variable called "source".**

## Calculate the total weight of trash collected by Professor Trash Wheel:
```{r total_prof}
mr_prof_gwy |> 
  filter(source == "professional_trash_wheel") |> 
  summarise(total_weight = sum(weight_tons, na.rm = TRUE))
```

## Calculate the total number of cigarette butts collected by Gwynnda in June of 2022:
```{r cig_gwy}
mr_prof_gwy |> 
  filter(source == "gwynnda_trash_wheel", year == "2022", month =="June")|> 
  summarise(number_cig = sum(cigarette_butts, na.rm = TRUE))
```

# Problem 3

## Load "bakers" dataset:
```{r bakers}
bakers = read_csv ("./data/gbb_datasets/bakers.csv") |> 
  janitor::clean_names() |> 
  separate(baker_name, into = c("first_name", "last_name"), sep = " ", extra = "merge")
```

## Load "bakes" dataset: 
```{r bakes}
bakes = read_csv ("./data/gbb_datasets/bakes.csv") |> 
  janitor::clean_names() |> 
  rename (
    first_name = "baker"
  )
```

## Combine the two datasets above:
```{r combined_bakers}
combined_bakers =
  left_join(bakes, bakers, by = c("first_name", "series")) |> 
  arrange (series, episode) |> 
  select (series, episode, first_name, last_name, baker_age, everything())
```

## Load the dataset "results":
```{r results}
results = 
  read_csv ("./data/gbb_datasets/results.csv", skip = 2) |> 
  mutate(
    result = case_match(
      result,
      "IN" ~"stayed_in",
      "OUT"~ "eliminated",
      "STAR BAKER" ~ "star_baker",
      "WINNER" ~ "series_winner",
      "Runner-up" ~ "series_runner-up",
      "WD" ~ "withdrew"
    )
  ) |> 
  mutate (
    star_baker = if_else (result == "star_baker", TRUE, NA),
    series_winner = if_else (result == "series_winner", TRUE, NA)
  ) |> 
  rename (
    first_name = "baker"
  )
```

## Combine to make the final dataset:
```{r final}
baker_final = 
  left_join (results, combined_bakers, by = c("first_name", "series", "episode")) |> 
  select (series, episode, first_name, last_name, baker_age, result, star_baker, series_winner, everything())
```
**In the data-cleaning process, I combined the first two datasets (bakers and bakes) by the common variables: first_name and series. Then, I combined all three datasets. Finally, the dataset contains 1136 observations and 13 variables.**

## create a table showing the star baker and winner of each episode:
```{r table}
baker_final |> 
  filter(series %in% 5:10) |> 
  filter(star_baker == TRUE | series_winner == TRUE) |> 
  select(series, episode, first_name, star_baker, series_winner) |> 
  knitr::kable()
```
**It is expected that individuals who win 'Star Baker' multiple times are more likely to become the 'Series Winner.' However, it is surprising that some contestants, like Richard, won 'Star Baker' several times but never won the 'Series Winner'**

## Load viewer dataset:
```{r viewer}
viewer = read_csv ("./data/gbb_datasets/viewers.csv") |> 
  janitor::clean_names() |> 
  pivot_longer(
    cols = series_1:series_10,
    names_to = "series_number",
    values_to = "viewership"
  )
head(viewer, 10)
```

## Calculate the average viewership in season 1:
```{r mean_season1}
viewer |> 
  filter (episode =="1") |> 
  summarise (viewership_mean_1 = mean (viewership))
```

## Calculate the average viewership in season 5:
```{r mean_season5}
viewer |> 
  filter (episode =="5") |> 
  summarise (viewership_mean_5 = mean (viewership))
```